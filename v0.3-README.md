# FusionLoom v0.3 - LLM Integration & System Information

Version 0.3 of FusionLoom introduces a fully functional LLM (Large Language Model) page that mimics the original AI-Garage application. This version brings multiple LLM providers into operation, allowing users to interact with various AI models through a unified interface. It also adds accurate system information detection similar to AI-Garage.

## New Features

- **Enhanced LLM Page**: Improved LLM page with a modern, responsive UI inspired by open-webui
- **Multiple LLM Providers**: Support for Ollama, Claude, ChatGPT, and Gemini
- **Local Services Tab**: Integration with web-based LLM services
- **Chat History**: Persistent chat history for each provider with search functionality
- **Model Selection**: Enhanced UI for selecting different models from each provider
- **File Attachments**: Support for attaching files to messages
- **Markdown Rendering**: Support for markdown formatting in messages
- **Model Availability Checking**: Ability to check available models for Ollama
- **Message Actions**: Copy, regenerate, and other actions for messages
- **Chat Export**: Export conversations as markdown files
- **Welcome Suggestions**: Quick-start suggestions for new chats
- **Accurate System Information**: Native-like system detection for CPU, GPU, RAM and OS information
- **Improved Status Bar**: Cleaner status bar with better error handling

## Architecture

The LLM implementation follows a modular architecture:

```
ui/
├── static/
│   ├── css/
│   │   ├── components/
│   │   │   ├── chat.css          # Chat interface styles
│   │   │   ├── message.css       # Message bubble styles
│   │   │   ├── model-selector.css # Model selection UI
│   │   │   └── service-tabs.css  # Service provider tabs
│   │   ├── main.css              # Main CSS entry point
│   │   └── fusionloom.css        # CSS entry point for Open WebUI
│   └── js/
│       ├── modules/
│       │   ├── llm/
│       │   │   ├── chat.js       # Core chat functionality
│       │   │   ├── history.js    # Chat history management
│       │   │   ├── ui.js         # LLM UI components
│       │   │   ├── ollama.js     # Ollama-specific implementation
│       │   │   ├── claude.js     # Claude-specific implementation
│       │   │   ├── chatgpt.js    # ChatGPT-specific implementation
│       │   │   └── gemini.js     # Gemini-specific implementation
│       │   └── ...
│       ├── main.js               # Main JavaScript entry point
│       └── fusionloom.js         # JS entry point for Open WebUI
└── index.html                    # Main HTML file

server/
├── system_info.py                # System information API
└── run_api.sh                    # Script to run the system info API
```

## Implementation Details

### LLM Providers

Each LLM provider is implemented as a separate module with consistent interfaces:

- **Ollama**: Local LLM server integration with model availability checking
- **Claude**: Anthropic's Claude API integration with Claude 3 models
- **ChatGPT**: OpenAI's GPT models integration including GPT-4o
- **Gemini**: Google's Gemini models integration including Gemini 1.5

### UI Components

The UI is built with a component-based approach:

- **Service Tabs**: Enhanced tabs for switching between different LLM providers with status indicators
- **Chat Interface**: Improved split view with collapsible history sidebar and main chat area
- **Model Selection**: Enhanced UI for selecting different models with search functionality
- **Message Bubbles**: Styled message bubbles with markdown rendering and action buttons
- **Typing Indicator**: Visual indicator when the model is generating a response
- **Welcome Screen**: Welcome message with quick-start suggestions

### Data Flow

1. User selects an LLM provider from the service tabs
2. User selects a model from the provider's model list
3. User enters a message in the input area
4. Message is sent to the selected provider's API
5. Response is displayed in the chat area with markdown rendering
6. Chat history is updated and persisted

## Configuration

The LLM providers can be configured in the Settings page:

- **API Endpoints**: URLs for each provider's API
- **API Keys**: Authentication keys for each provider
- **Default Models**: Default model selection for each provider

## Container Configuration

The UI files are now properly mounted directly to the build directory in the container:

```yaml
volumes:
  - ../../ui:/app/build
```

## Known Issues

- No support for streaming responses yet (will be added in a future version)
- Limited markdown rendering capabilities (will be enhanced in future versions)
- No support for multi-modal inputs like images (planned for future versions)

## Future Improvements

- Streaming responses for more interactive conversations
- Conversation threading and branching
- Advanced model parameters configuration
- Multi-modal inputs (images, audio)
- Prompt templates and saved prompts
- Integration with vector databases for RAG

## System Information API

The system information API provides accurate hardware and OS detection similar to AI-Garage:

- **CPU Detection**: Detects CPU model, manufacturer, and core count
- **GPU Detection**: Identifies GPU model and manufacturer
- **RAM Detection**: Reports total system memory
- **OS Detection**: Identifies operating system with distribution details
- **Architecture Detection**: Detects system architecture (x86_64, ARM, etc.)

The API is implemented as a Python Flask server that uses platform-specific methods to gather accurate system information. This approach overcomes the limitations of browser-based detection methods and provides native-like system information.

### Starting the System Information API

1. Navigate to the server directory:
   ```
   cd server
   ```

2. Run the API server:
   ```
   ./run_api.sh
   ```

3. The API will be available at `http://localhost:5050/api/system-info`

## Usage

1. Start the system information API:
   ```
   cd server
   ./run_api.sh
   ```

2. Start FusionLoom using the launch script:
   ```
   ./launch.sh
   ```

3. Navigate to the LLM page using the sidebar menu
4. Choose a model from the provider's model list
5. Start chatting with the selected model

## Requirements

- Ollama for local LLM support
- API keys for Claude, ChatGPT, and Gemini (optional)
- Python 3.6+ with Flask for the system information API
